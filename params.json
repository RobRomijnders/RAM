{
  "name": "RAM",
  "tagline": "Recurrent models of visual attention",
  "body": "## Recurrent model of visual attention\r\nThis post implement the Recurrent model of visual attention paper by [Mnih, 2014](http://arxiv.org/abs/1406.6247). Like the authors, we refer to this algorithm as Recurrent Attention model (RAM).\r\n\r\n## RAM\r\nThe RAM continues the work in attention models for images. Conventional approaches for image classification scale poorly with image size. Humans do not absorb images in a one-shot fashion. We scan the image and attend to part of interest. The RAM models this attention seeking.\r\n\r\nOur previous posts treat attention models too. The [DRAW attends to images via Gaussian filters](https://robromijnders.github.io/DRAW/). Another post shows [three implementations of attention using feature keys](https://robromijnders.github.io/attention/). In the RAM paper, the authors observe that humans make decisions on _where_ to look. Next, the eye focuses on a thumb-sizes patch. All other information gets blurred. \r\n\r\nThese processing forms the basis for RAM. The fovea-like extracts center around a point that conditions on the state of a network. The hidden state of an LSTM maps to a coordinate vector. The corresponding fovea-like extracts inputs to the LSTM at the next time-step.\r\n\r\nOur model of the fovea is non-differentiable. The stochasticity in the coordinate-vector allows us to use the REINFORCE rule. For this Reinforcement agent, the reward follows from a correct prediction after a fixed number of time-steps.\r\n\r\n## Results\r\nThe implementation builds upon [this work.](https://github.com/seann999/tensorflow_mnist_ram/blob/master/ram.py) Likewise, our visualization resembles his plots.\r\n![RAM_gif](https://github.com/RobRomijnders/RAM/blob/master/canvas/RAM_show.gif?raw=true)\r\nThe red squares highlight where the network centers its glimpses. I trained this network on an old laptop for a couple of hours. With more computing power, you might decrease the glimpse-size, allow for more time-steps. Those changes will make the RAM more resemble the human vision system.\r\nThe corresponding classification achieves 87% accuracy on MNIST\r\n\r\nAs always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}